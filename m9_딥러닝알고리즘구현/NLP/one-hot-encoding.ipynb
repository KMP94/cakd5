{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"one-hot-encoding.ipynb","provenance":[],"authorship_tag":"ABX9TyNDjk5Ndar6sfiLU1/bKkSU"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["- 원-핫 인코딩은 토큰을 벡터로 변환하는 가장 일반적이고 기본적인 방법입니다. \n","- 모든 단어에 고유한 정수 인덱스를 부여하고 이 정수 인덱스 i를 크기가 N(어휘 사전의 크기)인 이진 벡터로 변환합니다. \n","- 이 벡터는 i번째 원소만 1이고 나머지는 모두 0입니다.\n","\n","- 물론 원-핫 인코딩은 문자 수준에서도 적용할 수 있습니다.\n"],"metadata":{"id":"I1ZldFPcWG2n"}},{"cell_type":"code","source":["import numpy as np\n","samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n","token_index = {}\n","for sample in samples:\n","  for word in sample.split():\n","    if word not in token_index:\n","      token_index[word] = len(token_index) + 1\n","\n","max_length = 10\n","results = np.zeros((len(samples),max_length, max(token_index.values()) + 1))\n","for i,sample in enumerate(samples):\n","  for j,word in list(enumerate(sample.split()))[:max_length]:\n","    index = token_index.get(word)\n","    results[i, j, index] = 1."],"metadata":{"id":"H0L5VVOWXFWc","executionInfo":{"status":"ok","timestamp":1649811054138,"user_tz":-540,"elapsed":434,"user":{"displayName":"kwangmin PARK","userId":"07295163492788646147"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["results[1]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cDXbWdPeY19K","executionInfo":{"status":"ok","timestamp":1649812134942,"user_tz":-540,"elapsed":3,"user":{"displayName":"kwangmin PARK","userId":"07295163492788646147"}},"outputId":"f018b1b3-8c08-4d81-d5b7-5462c369db71"},"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n","       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n","       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n","       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n","       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n","       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n","       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n","       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n","       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n","       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["token_index"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IlNwAbLGYFGy","executionInfo":{"status":"ok","timestamp":1649811179392,"user_tz":-540,"elapsed":318,"user":{"displayName":"kwangmin PARK","userId":"07295163492788646147"}},"outputId":"9fd74d5e-90e4-4165-9f4a-075669e4b068"},"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'The': 1,\n"," 'ate': 8,\n"," 'cat': 2,\n"," 'dog': 7,\n"," 'homework.': 10,\n"," 'mat.': 6,\n"," 'my': 9,\n"," 'on': 4,\n"," 'sat': 3,\n"," 'the': 5}"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["from typing import Sequence\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","\n","samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n","tokenizer = Tokenizer(num_words=20)\n","\n","# 단어 인덱스를 구축\n","tokenizer.fit_on_texts(samples)\n","\n","# 정수 인덱스 리스트로 변환\n","sequences = tokenizer.texts_to_sequences(samples)\n","\n","# 원-핫 이진 벡터 표현\n","one_hot_results = tokenizer.texts_to_matrix(samples,mode='binary')\n","\n","word_index = tokenizer.word_index\n","print(word_index)\n","print(sequences)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QJYh1kGfdqZN","executionInfo":{"status":"ok","timestamp":1649812968420,"user_tz":-540,"elapsed":3,"user":{"displayName":"kwangmin PARK","userId":"07295163492788646147"}},"outputId":"e875ed99-1882-424c-8113-79e39e9be754"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["{'the': 1, 'cat': 2, 'sat': 3, 'on': 4, 'mat': 5, 'dog': 6, 'ate': 7, 'my': 8, 'homework': 9}\n","[[1, 2, 3, 4, 1, 5], [1, 1, 1, 6, 7, 8, 9]]\n"]}]},{"cell_type":"markdown","source":["영어 5개 문장으로 구성된 텍스트를 가져와서 다음을 수행하세요.\n","- 수작업으로 벡터화\n","- keras를 사용하여 벡터화"],"metadata":{"id":"xYnE2FFxhWVd"}},{"cell_type":"code","source":["samples = ['Goal setting is the secret to a compelling future.', 'To know how much there is to know is the beginning of learning to live.' ,\n","           'Learn as if you will live forever, live like you will die tomorrow.', 'When you change your thoughts, remember to also change your world.', 'I never dreamed about success. I worked for it.']"],"metadata":{"id":"bsV4XBFu-NO9","executionInfo":{"status":"ok","timestamp":1649820822254,"user_tz":-540,"elapsed":2,"user":{"displayName":"kwangmin PARK","userId":"07295163492788646147"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","\n","token_index = {}\n","for sample in samples:\n","  for word in sample.split():\n","    if word not in token_index:\n","      token_index[word] = len(token_index) + 1\n","\n","max_length = 100\n","results = np.zeros((len(samples),max_length, max(token_index.values()) + 1))\n","for i,sample in enumerate(samples):\n","  for j,word in list(enumerate(sample.split()))[:max_length]:\n","    index = token_index.get(word)\n","    results[i, j, index] = 1."],"metadata":{"id":"9PlgzP88-XY9","executionInfo":{"status":"ok","timestamp":1649820828056,"user_tz":-540,"elapsed":389,"user":{"displayName":"kwangmin PARK","userId":"07295163492788646147"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["print(token_index)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sehzIake-_qO","executionInfo":{"status":"ok","timestamp":1649820929733,"user_tz":-540,"elapsed":250,"user":{"displayName":"kwangmin PARK","userId":"07295163492788646147"}},"outputId":"cacb565c-e704-4574-8078-f99b511d66ba"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["{'Goal': 1, 'setting': 2, 'is': 3, 'the': 4, 'secret': 5, 'to': 6, 'a': 7, 'compelling': 8, 'future.': 9, 'To': 10, 'know': 11, 'how': 12, 'much': 13, 'there': 14, 'beginning': 15, 'of': 16, 'learning': 17, 'live.': 18, 'Learn': 19, 'as': 20, 'if': 21, 'you': 22, 'will': 23, 'live': 24, 'forever,': 25, 'like': 26, 'die': 27, 'tomorrow.': 28, 'When': 29, 'change': 30, 'your': 31, 'thoughts,': 32, 'remember': 33, 'also': 34, 'world.': 35, 'I': 36, 'never': 37, 'dreamed': 38, 'about': 39, 'success.': 40, 'worked': 41, 'for': 42, 'it.': 43}\n"]}]},{"cell_type":"code","source":["results[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gkxx_BeY_ZOB","executionInfo":{"status":"ok","timestamp":1649820936231,"user_tz":-540,"elapsed":2,"user":{"displayName":"kwangmin PARK","userId":"07295163492788646147"}},"outputId":"1f585ca4-facf-4416-f2d4-423050222077"},"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0., 1., 0., ..., 0., 0., 0.],\n","       [0., 0., 1., ..., 0., 0., 0.],\n","       [0., 0., 0., ..., 0., 0., 0.],\n","       ...,\n","       [0., 0., 0., ..., 0., 0., 0.],\n","       [0., 0., 0., ..., 0., 0., 0.],\n","       [0., 0., 0., ..., 0., 0., 0.]])"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["from typing import Sequence\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","\n","tokenizer = Tokenizer(num_words=100)\n","\n","# 단어 인덱스를 구축\n","tokenizer.fit_on_texts(samples)\n","# 원-핫 이진 벡터 표현\n","one_hot_results = tokenizer.texts_to_matrix(samples,mode='binary')\n","\n","print(one_hot_results)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2XIg6xJ__eAX","executionInfo":{"status":"ok","timestamp":1649820996747,"user_tz":-540,"elapsed":230,"user":{"displayName":"kwangmin PARK","userId":"07295163492788646147"}},"outputId":"a5ff1526-ee4d-406e-87ef-b7a9fd933c5c"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["[[0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0.\n","  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n","  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n","  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n","  0. 0. 0. 0.]\n"," [0. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0.\n","  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n","  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n","  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n","  0. 0. 0. 0.]\n"," [0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n","  1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n","  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n","  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n","  0. 0. 0. 0.]\n"," [0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n","  0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n","  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n","  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n","  0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n","  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0.\n","  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n","  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n","  0. 0. 0. 0.]]\n"]}]}]}